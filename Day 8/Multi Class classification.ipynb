{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering, customer Complains\n",
    "-[Rishit Dagli](rishit.tech)\n",
    "\n",
    "## About Me\n",
    "\n",
    "[Twitter](https://twitter.com/rishit_dagli)\n",
    "\n",
    "[GitHub](https://github.com/Rishit-dagli)\n",
    "\n",
    "[Medium](https://medium.com/@rishit.dagli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "6c3c4146-8473-41e2-a662-413efb27665a",
    "_uuid": "637cff8e2103e323ef609bd2c350cb29aa05e099"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras import utils as np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "We will also make the x and y splits now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "1da17257-6c48-42fe-b761-15148be8a56a",
    "_uuid": "ddb47654b861d29d92251edeed58352f05490e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Debt collection' 'Consumer Loan' 'Mortgage' ... 'Payday loan' 'Mortgage'\n",
      " 'Mortgage']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Consumer Finance Complaints/consumer_complaints.csv\", \n",
    "                usecols=('product','consumer_complaint_narrative'),\n",
    "                dtype={'consumer_complaint_narrative': object})\n",
    "\n",
    "data=data[data['consumer_complaint_narrative'].notnull()]\n",
    "\n",
    "data=data[data['product'].notnull()]\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "x = data.iloc[:, 1].values\n",
    "y = data.iloc[:, 0].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the unique categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['Bank account or service', 'Consumer Loan', 'Credit card',\n",
      "       'Credit reporting', 'Debt collection', 'Money transfers',\n",
      "       'Mortgage', 'Other financial service', 'Payday loan',\n",
      "       'Prepaid card', 'Student loan'], dtype=object), array([ 5711,  3678,  7929, 12526, 17552,   666, 14919,   110,   726,\n",
      "         861,  2128]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Tokenize words and remove some values with TF Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "432b95a1-0703-4279-a5e2-3f4797ba9854",
    "_uuid": "077100d4dd6995a3f78f183057055969e2808927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   3  84 108]\n",
      " [  0   0   0 ...   2   8   6]\n",
      " [145  10 112 ...   7   9   7]\n",
      " ...\n",
      " [  0   0   0 ... 171  66   1]\n",
      " [  0   0   0 ...   2 150  68]\n",
      " [ 32   2   4 ...   5  24  16]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words= 200, filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True, split=\" \")\n",
    "tokenizer.fit_on_texts(x)\n",
    "x = tokenizer.texts_to_sequences(x)\n",
    "x = sequence.pad_sequences(x, maxlen=200)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert categorical values to numerical identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "9316d3ac-cb03-421e-8d81-225238d13978",
    "_uuid": "80b7c94682a711491f8bdf574c382c8f156024dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 6 ... 8 6 6]\n"
     ]
    }
   ],
   "source": [
    "labelencoder_Y = LabelEncoder()\n",
    "y = labelencoder_Y.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the unique values we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([ 5711,  3678,  7929, 12526, 17552,   666, 14919,   110,   726,\n",
      "         861,  2128]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also convert Y to a form like this\n",
    "\n",
    "Class 1 = [0, 0, 0, 1]\n",
    "\n",
    "Class 2 = [1, 0, 0, 1]\n",
    "\n",
    "Aas ana example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "8a79ea2e-8033-4919-b376-9ace5c02afab",
    "_uuid": "f9c775c1d7793986af517cf922df277e9414a221",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y = np_utils.to_categorical(y, num_classes= 11)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "3a817ac7-7adf-404b-becf-5f2a0a9d6d05",
    "_uuid": "af2812f73524c6fe28fa276bc5c67b0db5110ee1"
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "indices = np.arange(len(x))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now remove some bad characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_from=3\n",
    "start_char = 1\n",
    "if start_char is not None:\n",
    "        x = [[start_char] + [w + index_from for w in x1] for x1 in x]\n",
    "elif index_from:\n",
    "        x = [[w + index_from for w in x1] for x1 in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to deal with OOV or out of vocabulary terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53444, 201)\n",
      "(53444, 11)\n",
      "(13362, 201)\n",
      "(13362, 11)\n"
     ]
    }
   ],
   "source": [
    "num_words = None\n",
    "if not num_words:\n",
    "        num_words = max([max(x1) for x1 in x])\n",
    "        \n",
    "oov_char = 2\n",
    "skip_top = 0\n",
    "\n",
    "if oov_char is not None:\n",
    "        x = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in x]\n",
    "else:\n",
    "        x = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in x]\n",
    "        \n",
    "test_split = 0.2\n",
    "idx = int(len(x) * (1 - test_split))\n",
    "x_train, y_train = np.array(x[:idx]), np.array(y[:idx])\n",
    "x_test, y_test = np.array(x[idx:]), np.array(y[idx:])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our sequences of equal length we will pad them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (53444, 201)\n",
      "x_test shape: (13362, 201)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=201)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=201)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define a few hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "maxlen = 201\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(11))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53444 samples, validate on 13362 samples\n",
      "Epoch 1/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.9051 - accuracy: 0.7013 - val_loss: 0.8811 - val_accuracy: 0.7098\n",
      "Epoch 2/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8354 - accuracy: 0.7251 - val_loss: 0.8502 - val_accuracy: 0.7225\n",
      "Epoch 3/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.8064 - accuracy: 0.7361 - val_loss: 0.7868 - val_accuracy: 0.7441\n",
      "Epoch 4/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.7887 - accuracy: 0.7438 - val_loss: 0.8053 - val_accuracy: 0.7426\n",
      "Epoch 5/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.7824 - accuracy: 0.7477 - val_loss: 0.8140 - val_accuracy: 0.7364\n",
      "Epoch 6/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.7810 - accuracy: 0.7492 - val_loss: 0.7979 - val_accuracy: 0.7438\n",
      "Epoch 7/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.7859 - accuracy: 0.7482 - val_loss: 0.8507 - val_accuracy: 0.7334\n",
      "Epoch 8/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.7895 - accuracy: 0.7486 - val_loss: 0.8094 - val_accuracy: 0.7375\n",
      "Epoch 9/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.7951 - accuracy: 0.7484 - val_loss: 0.8529 - val_accuracy: 0.7378\n",
      "Epoch 10/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8030 - accuracy: 0.7451 - val_loss: 0.8451 - val_accuracy: 0.7285\n",
      "Epoch 11/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.8070 - accuracy: 0.7439 - val_loss: 0.8541 - val_accuracy: 0.7339\n",
      "Epoch 12/50\n",
      "53444/53444 [==============================] - 19s 357us/sample - loss: 0.8139 - accuracy: 0.7432 - val_loss: 0.8417 - val_accuracy: 0.7340\n",
      "Epoch 13/50\n",
      "53444/53444 [==============================] - 18s 340us/sample - loss: 0.8190 - accuracy: 0.7452 - val_loss: 0.8515 - val_accuracy: 0.7290\n",
      "Epoch 14/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8250 - accuracy: 0.7421 - val_loss: 0.9252 - val_accuracy: 0.7128\n",
      "Epoch 15/50\n",
      "53444/53444 [==============================] - 19s 350us/sample - loss: 0.8268 - accuracy: 0.7418 - val_loss: 0.8357 - val_accuracy: 0.7354\n",
      "Epoch 16/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8299 - accuracy: 0.7421 - val_loss: 0.8502 - val_accuracy: 0.7353\n",
      "Epoch 17/50\n",
      "53444/53444 [==============================] - 18s 340us/sample - loss: 0.8423 - accuracy: 0.7401 - val_loss: 0.8773 - val_accuracy: 0.7314\n",
      "Epoch 18/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8415 - accuracy: 0.7400 - val_loss: 0.9187 - val_accuracy: 0.7042\n",
      "Epoch 19/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.8475 - accuracy: 0.7369 - val_loss: 0.8737 - val_accuracy: 0.7320\n",
      "Epoch 20/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8569 - accuracy: 0.7352 - val_loss: 0.8886 - val_accuracy: 0.7249\n",
      "Epoch 21/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8588 - accuracy: 0.7354 - val_loss: 0.8784 - val_accuracy: 0.7315\n",
      "Epoch 22/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8731 - accuracy: 0.7322 - val_loss: 0.8663 - val_accuracy: 0.7237\n",
      "Epoch 23/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8788 - accuracy: 0.7300 - val_loss: 0.8636 - val_accuracy: 0.7329\n",
      "Epoch 24/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8895 - accuracy: 0.7298 - val_loss: 0.8860 - val_accuracy: 0.7243\n",
      "Epoch 25/50\n",
      "53444/53444 [==============================] - 18s 340us/sample - loss: 0.8888 - accuracy: 0.7279 - val_loss: 0.8689 - val_accuracy: 0.7294\n",
      "Epoch 26/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.8978 - accuracy: 0.7266 - val_loss: 0.9592 - val_accuracy: 0.6997\n",
      "Epoch 27/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.9076 - accuracy: 0.7287 - val_loss: 0.8795 - val_accuracy: 0.7288\n",
      "Epoch 28/50\n",
      "53444/53444 [==============================] - 18s 340us/sample - loss: 0.9029 - accuracy: 0.7250 - val_loss: 0.9089 - val_accuracy: 0.7213\n",
      "Epoch 29/50\n",
      "53444/53444 [==============================] - 19s 355us/sample - loss: 0.9139 - accuracy: 0.7234 - val_loss: 0.8974 - val_accuracy: 0.7254\n",
      "Epoch 30/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.9226 - accuracy: 0.7191 - val_loss: 0.9316 - val_accuracy: 0.7147\n",
      "Epoch 31/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.9293 - accuracy: 0.7185 - val_loss: 0.9241 - val_accuracy: 0.7314\n",
      "Epoch 32/50\n",
      "53444/53444 [==============================] - 19s 351us/sample - loss: 0.9309 - accuracy: 0.7176 - val_loss: 0.9327 - val_accuracy: 0.7143\n",
      "Epoch 33/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.9385 - accuracy: 0.7170 - val_loss: 0.9266 - val_accuracy: 0.7200\n",
      "Epoch 34/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.9503 - accuracy: 0.7164 - val_loss: 0.9295 - val_accuracy: 0.7185\n",
      "Epoch 35/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.9470 - accuracy: 0.7164 - val_loss: 0.9485 - val_accuracy: 0.7128\n",
      "Epoch 36/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.9585 - accuracy: 0.7134 - val_loss: 0.9266 - val_accuracy: 0.7166\n",
      "Epoch 37/50\n",
      "53444/53444 [==============================] - 18s 340us/sample - loss: 0.9674 - accuracy: 0.7117 - val_loss: 0.9307 - val_accuracy: 0.7209\n",
      "Epoch 38/50\n",
      "53444/53444 [==============================] - 18s 343us/sample - loss: 0.9699 - accuracy: 0.7110 - val_loss: 0.9247 - val_accuracy: 0.7173\n",
      "Epoch 39/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.9680 - accuracy: 0.7101 - val_loss: 0.9352 - val_accuracy: 0.7164\n",
      "Epoch 40/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.9703 - accuracy: 0.7081 - val_loss: 0.9594 - val_accuracy: 0.7190\n",
      "Epoch 41/50\n",
      "53444/53444 [==============================] - 18s 340us/sample - loss: 0.9749 - accuracy: 0.7092 - val_loss: 0.9512 - val_accuracy: 0.7216\n",
      "Epoch 42/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.9757 - accuracy: 0.7104 - val_loss: 0.9833 - val_accuracy: 0.6902\n",
      "Epoch 43/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.9862 - accuracy: 0.7089 - val_loss: 0.9700 - val_accuracy: 0.7045\n",
      "Epoch 44/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 0.9889 - accuracy: 0.7054 - val_loss: 0.9290 - val_accuracy: 0.7179\n",
      "Epoch 45/50\n",
      "53444/53444 [==============================] - 18s 341us/sample - loss: 0.9938 - accuracy: 0.7038 - val_loss: 0.9813 - val_accuracy: 0.7078\n",
      "Epoch 46/50\n",
      "53444/53444 [==============================] - 19s 357us/sample - loss: 1.0052 - accuracy: 0.7029 - val_loss: 0.9996 - val_accuracy: 0.6961\n",
      "Epoch 47/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 1.0154 - accuracy: 0.7042 - val_loss: 0.9924 - val_accuracy: 0.6980\n",
      "Epoch 48/50\n",
      "53444/53444 [==============================] - 18s 345us/sample - loss: 1.0273 - accuracy: 0.6950 - val_loss: 0.9826 - val_accuracy: 0.7006\n",
      "Epoch 49/50\n",
      "53444/53444 [==============================] - 19s 347us/sample - loss: 1.0212 - accuracy: 0.6993 - val_loss: 0.9817 - val_accuracy: 0.7110\n",
      "Epoch 50/50\n",
      "53444/53444 [==============================] - 18s 342us/sample - loss: 1.0327 - accuracy: 0.6969 - val_loss: 0.9779 - val_accuracy: 0.7070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd7101e7978>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          batch_size=32,\n",
    "          epochs=50,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approximate peak of 75 % is good atleast for a dataset like this which has a lot of noise, so we did good and also did no overfitting this is good for now. We first tokenized our text and then removed the bad characters too. With some hyper parameter tuning we got the best model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
